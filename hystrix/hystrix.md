分布式系统中经常会出现某个基础服务不可用造成整个系统不可用的情况，这种现象被称为服务雪崩效应，为了应对服务雪崩效应，一种惯用的做法就是对服务做降级。而Hystrix就是我们做服务降级的一种选择

---

##### 服务雪崩效应

什么是服务雪崩？

服务雪崩效应是一种因服务提供者的不可用导致服务调用者的不可用，并将这种不可用逐渐放大镜的过程。好下图所示:A为服务提供者,B为A的服务调用者,C和D是B的服务调用者,当A不可用时，引起B不可用，并将不可用逐渐放大到C和D时，服务雪崩就形成了.

![服务雪崩示意图](https://github.com/linkiChen/SpringCloud-2.1.0-toturials/blob/master/imgs/hystrix-1.jpg)

形成原因：

服务雪崩的形成原因可分为三个阶段:

1. 服务提供者不可用
2. 重试加大流量
3. 服务调用者不可用

造成==服务不可用==的原因可能有:

- 硬件故障
- 程序bug
- 缓存击穿
- 用户大师请求

硬件故障可能为硬件损坏造成的服务器主机宕机，网络硬件故障造成的服务提供者的不可访问。

缓存击密穿一般发生在缓存应用重启，所有缓存被清空时，以及短时间内大量缓存失效时,大量的缓存不命中，请求直接走后台接口造成服务提供者超负荷运行，引起服务不可用。

在秒杀和大跌开始前,如果准备不充分，用户发起大量请求也会生成服务提供者的不可用.

而形成==重试加大流量==的原因有:

- 用户重试
- 代码逻辑重试

在服务提供者不可用后，用户由于忍受不了界面上长时间的等待，而不断刷新页面甚至提交表单。服务调用端会存在大量服务异常后的重试逻辑。

这些重试都会进一步加大请求流量。

==服务调用者不可用==产生的主要原因是:

- 同步等待造成的资源耗尽

当服务调用者使用同步调用时，会产生大量的等待线程占用系统资源，一旦线程资源被耗尽，服务调用者提供的服务也将处于不可用状态，于是服务雪崩效应产生。



应对策略：

针对造成服务雪崩的不同原因，可以使用不同的应对策略：

1. 流量控制
2. 改进缓存模式
3. 服务自动扩容
4. 服务调用者降级服务

==流量控制==的具体措施包括:

- 网关限流
- 用户交互限流
- 关闭重试

因为Nginx的高性能，目前一线互联网公司大量采用Nignx+Lua的网关进行流量控制，由此而来的OpenResty也越来越热门。

用户交互限流的具体措施有:

1. 采用加载动画,提高用户的忍耐等待时间

2. 提交按钮添加强制等待时间机制

==改进缓存模式==的措施包括:

- 缓存预加载
- 同步改为异步刷新

==服务调用者降级服务==的措施包括:

- 资源隔离
- 对依赖服务进行分类
- 不可用服务的调用快速失败

资源隔离主要是对调用服务的线程池进行隔离

根据具体业务，将依赖服务分为强依赖和弱依赖。强依赖不可用会导致当前业务中止，而弱依赖服务的不可用不会导致当前业务的中止

不可用服务的调用快速失败一般通过**超时机制**、**熔断器**和熔断后的**降级方法**来实现

---

##### 使用Hystrix预防服务雪崩

**服务降级(Fallback)**

对于查询操作，可以实现一个fallback方法，当请求后端服务出现异常的时候，可以使用fallback方法返回的值，fallback方法的返回值一般是设置默认值或者来自缓存

**资源隔离**

在Hystrix中主要有两种方式来实现资源的隔离：**线程池**隔离和**信号量**隔离.

对于线程池隔离，通常在使用的时候会根据调用的远程服务划分出多个线程池。例如调用产品服务的Command放入A线程池,用户账户服务的Command放入B线程池。这样做的主要优点是运行环境被隔离开。这样就算调用服务的代码存在bug或者由于其他原因导致自己所在线程池被耗尽时，不会对系统的其他服务造成影响。

通过对依赖服务的**线程池**实现隔离，可以带来如下优势：

- 应用自身得到完全的保护，不会受不可控的依赖服务影响。即便给依赖服务分配的线程池被填满，也不会影响应用自身的额其余部分。
- 可以有效的降低接入新服务的风险。如果新服务接入后运行不稳定或存在问题，完全不会影响到应用其他的请求。
- 当依赖的服务从失效恢复正常后，它的线程池会被清理并且能够马上恢复健康的服务，相比之下容器级别的清理恢复速度要慢得多。
- 当依赖的服务出现配置错误的时候，线程池会快速的反应出此问题（通过失败次数、延迟、超时、拒绝等指标的增加情况）。同时，我们可以在不影响应用功能的情况下通过实时的动态属性刷新（后续会通过 Spring Cloud Config 与 Spring Cloud Bus 的联合使用来介绍）来处理它。
- 当依赖的服务因实现机制调整等原因造成其性能出现很大变化的时候，此时线程池的监控指标信息会反映出这样的变化。同时，我们也可以通过实时动态刷新自身应用对依赖服务的阈值进行调整以适应依赖方的改变。
- 除了上面通过线程池隔离服务发挥的优点之外，每个专有线程池都提供了内置的并发实现，可以利用它为同步的依赖服务构建异步的访问。

总之，通过对依赖服务实现线程池隔离，让应用更健壮，不会因为个别依赖服务出现总量而引起非相关服务的异常。同时，也使得我们的应用变得更加灵活，可以在不停止服务的情况下，配合动态刷新实现性能配置上的调整。

除了线程池隔离之外，还可以使用**信号量**来控制单个依赖服务的并发度，信号量的开销要无比线程池的开销小得多，但是==它不能设置超时和实现异步访问==。所以，只有在依赖服务是足够可行的情况下才使用信号量。在HysrixCommand和HystrixObservableCommand中2处支持信号量的使用:

- 命令执行：如果隔离策略参数execution.isolation.strategy设置为SEMAPHORE,Hystrix会使用信号量替代线程池来控制依赖服务的并发控制
- 降级逻辑：当Hystrix尝试降级逻辑的时候，它会在调用线程中使用信息量

信号量的默认值为 10，我们也可以通过动态刷新配置的方式来控制并发线程的数量。对于信号量大小的估算方法与线程池并发度的估算类似。仅访问内存数据的请求一般耗时在 1ms 以内，性能可以达到 5000rps，这样级别的请求我们可以将信号量设置为 1 或者 2，我们可以按此标准并根据实际请求耗时来设置信号量。